<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: System-design | euccas.github.io]]></title>
  <link href="http://euccas.github.io/categories/system-design/atom.xml" rel="self"/>
  <link href="http://euccas.github.io/"/>
  <updated>2022-05-30T18:17:35-07:00</updated>
  <id>http://euccas.github.io/</id>
  <author>
    <name><![CDATA[euccas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Read Continuous Delivery With Spinnaker]]></title>
    <link href="http://euccas.github.io/blog/20220305/read-continuous-delivery-with-spinnaker.html"/>
    <updated>2022-03-05T22:30:58-08:00</updated>
    <id>http://euccas.github.io/blog/20220305/read-continuous-delivery-with-spinnaker</id>
    <content type="html"><![CDATA[<p><a href="https://spinnaker.io/">Spinnaker</a> is an open-source, multi-cloud continuous delivery platform originally developed by Netflix. Today, Spinnaker has built a community and many companies adopted it to power their Continous Delivery. Pinterest also uses Spinnaker to deploy some of its core services, including web and API. Recently I read this eBook: <a href="https://spinnaker.io/docs/concepts/ebook/">Continuous Delivery with Spinnaker</a>. What I like about this short eBook is that it explains the key design considerations of Spinnaker, and those are, as I think of them, what really matter when designing a good cloud CD platform. In this post, I will share a few topics mentioned in this eBook together with my thoughts after reading.</p>

<h2 id="cloud-deployment-considerations">Cloud Deployment Considerations</h2>

<p>Important things to consider:</p>

<ul>
  <li>Credentials management</li>
  <li>Regional isolation</li>
  <li>Autoscaling</li>
  <li>Immutable infrastructure and data persistence</li>
  <li>Service discovery</li>
  <li>Using multiple clouds</li>
  <li>Abstracting cloud operations from users</li>
</ul>

<!--more-->

<p>When you work on designing a CD platform, pay attention to where do you focus on. When I joined Pinterest back in 2019, I was working on the Continuous Delivery Platform team initially and I started designing Pinterest’s new CD platform. For that project, the team and I put a lot of focus on the developer experience and making the new system easy to use. While I think that was good, I also think we should have put more thoughts into the other areas such as credentials management, autoscaling, deploy policy support, different ways of triggering a deployment, in the design phase.</p>

<h2 id="structuring-deployments-as-pipelines">Structuring Deployments as Pipelines</h2>

<p>Benefits of flexible user-defined pipelines: allowing each team to build and maintain their own deployment pipeline from the building blocks the platform provides lets engineers experiment freely according to their needs.</p>

<p>Encapsulate the built-in features as platform defined pipeline stages:</p>

<ul>
  <li>Infrastructure stages: operate on the underlying cloud infrastructure by creating, updating, or deleting resources.</li>
  <li>External systems integration stages: examples are integrations with CI (Jenkins/Travis CI), run job, webhook.</li>
  <li>Testing stages: Chaos automation platform, Canary + ACA</li>
  <li>Controlling flow stages: allows to control the flow of pipelines, whether that is authorization, timing, or branching logic.</li>
  <li>Triggers stages: controls how a pipeline is started, eg. time-based triggers, event-based triggers.</li>
</ul>

<p>Continuous delivery is a complex process. I think using <em>pipeline</em> and <em>stage</em> as the two core concepts in Spinnaker’s design is an awesome idea. It abstracts the complexity of various type of deployments, and allows enough flexibility and extensibility by having both the <em>managed stages</em> and <em>customized stages</em>. On a side note, <a href="https://airflow.apache.org/">Apache Airflow</a>, the data pipeline orchestration system uses a similar principle in its design by providing <em>operators</em>. I may delve into the design of Airflow in another post later.</p>

<h2 id="working-with-cloud-vms-and-kubernetes">Working with Cloud VMs and Kubernetes</h2>

<p>For continuous deployment into Amazon’s EC2 virtual machine–based cloud, Spinnaker models a well-known set of operations as pipeline stages. Other VMbased cloud providers have similar functionality. Those operations mainly include:</p>

<ul>
  <li>Baking AMIs</li>
  <li>Tagging AMIs</li>
  <li>Deploying in EC2</li>
  <li>Availability zones</li>
  <li>Health checks</li>
  <li>Autoscaling</li>
</ul>

<p>Kubernetes makes deployment to the cloud much easier because of some of its advantages comparing to the VM-based cloud platform:</p>

<ul>
  <li>Faster: provisioning resources in Kubernetes takes seconds, while provisioning a VM can take minutes.</li>
  <li>Declarative: Kubernetes uses manifest files (YAML) to provide a declarative description of your infrastructure, and this is central to how Kubernetes works.</li>
  <li>Multi-cloud: whether Kubernetes is running in Google’s cloud or Amazon’s, in your on-premise datacenter or on your laptop, it exposes the same interface and behavior for running your workloads. This makes it trivial to deploy the same application to multiple clouds and regions, when you can treat each as being identical.</li>
  <li>Native deployment orchestration: when a change is submitted to a running Kubernetes workload, it orchestrates a rollout of your change according to policies you specify. In some cases, this becomes the only deployment orchestration that you need.</li>
</ul>

<p>Pinterest has an internal deployment system <a href="https://github.com/pinterest/teletraan">Teletraan</a>, and it was designed and used for deploying services to VMs (Amazon EC2). After joining Pinterest, I learned that a major user pain point of Teletraan was the complicated configurations needed for setting up a deployment environment for user’s services. For example, in Teletraan users need to configure AMI, place AZs, etc. I agree that Teletraan’s UI can be improved to make the experience better, but I think this problem is a result of the complexity of VM deployment. To solve it, we could either choose to move to Kubernetes, or redesign some parts of Teletraan to have a better abstraction model, similar to what Spinnaker did. In short, reducing the complexity in developer experience cannot be done just by fixing the UI.</p>

<p>Right now I’m no longer working on the Continuous delivery platform, but I still like to think about the problems and solutions in this area because I used to work on it, built a new system from scratch when I didn’t really have much knowledge in the cloud deployment space, had many questions and learned lessons (and got some wins too). If you happen to read this and want to continue the discussion with me on a specific topic, please feel free to drop me a line.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reading Notes - Designing Distributed Systems]]></title>
    <link href="http://euccas.github.io/blog/20180513/read-designing-distributed-systems.html"/>
    <updated>2018-05-13T18:22:12-07:00</updated>
    <id>http://euccas.github.io/blog/20180513/read-designing-distributed-systems</id>
    <content type="html"><![CDATA[<p>Recently I read a book <a href="http://shop.oreilly.com/product/0636920072768.do">Designing Distributed Systems</a>, which is written by <a href="https://twitter.com/brendandburns">Brendan Burns</a>, and published by O’Reilly earlier this year. This book introduces the patterns and components used in the development of distributed systems.</p>

<p>Before I started to read this book, I had three questions in my mind, and try to find the answers from the book. Those three questions are:</p>

<ol>
  <li>What’s the most important difference between designing distributed systems and single machine systems?</li>
  <li>Why container technology, such as docker, kubernates, is so popular? How could it be helpful?</li>
  <li>What are the common patterns used in distributed systems design, and when shall I use them?</li>
</ol>

<p>This book does give me the answers, at least partial ones. I put my reading notes into a Google Slides, and  <a href="https://docs.google.com/presentation/d/1srX9hRS9tbtrEx7T1abxbHiD1gASkvllf1_W2SjuadA/edit?usp=sharing">you can find it here to read the details</a>. A PDF version in light background color is <a href="https://github.com/euccas/euccas.github.io/blob/source/data/read-2018-design_distributed_systems_lightver.pdf">available here</a>.</p>

<p>The short answers to my questions are as in the following:</p>

<!--more-->

<h2 id="whats-the-most-important-difference-between-designing-distributed-systems-and-single-machine-systems">What’s the most important difference between designing distributed systems and single machine systems?</h2>

<ul>
  <li>Designing distributed systems can be significantly more complicated to design, build, and debug correctly.</li>
  <li>Designing distributed systems need much more efforts in designing for scalability and reliability.</li>
  <li>In a distributed system, tasks/data are spreaded to multiple workers. It requires techniques like containers and load balancing to utilize parallelisation</li>
</ul>

<h2 id="why-container-technology-docker-kubernetes-is-so-popular-how-could-they-be-helpful">Why container technology (docker, kubernetes) is so popular? How could they be helpful?</h2>

<ul>
  <li>Containers are not only useful for applications which have components running on multiple machines, but also for single machine applications.</li>
  <li>The goal of containerization is to <strong>establish boundaries</strong> around specific resources, team ownership, separation of concerns.</li>
  <li>The benefits include <strong>resource isolation</strong>, <strong>scaling teams</strong>, <strong>reuse components and modules</strong>, <strong>break big problems</strong> into smaller ones (Small, focused applications are easier to understand, be tested, updated and deployed)</li>
</ul>

<h2 id="what-are-the-common-patterns-used-in-distributed-systems-design-and-when-shall-i-use-them">What are the common patterns used in distributed systems design, and when shall I use them?</h2>

<p>The book describes three types of patterns.</p>

<ul>
  <li><strong>Single node</strong> patterns: sidecar, ambassadors, adapters</li>
  <li><strong>Serving</strong> patterns: sharded services, scatter/gather, FaaS, etc.</li>
  <li><strong>Batch computational</strong> patterns: Work queue, Event-driven batch processing, coordinated batch processsing, etc.</li>
</ul>

<p>You can find more detailed description of each design pattern in my reading notes.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Facebook Infrastructure: Streaming Video Engine (SVE)]]></title>
    <link href="http://euccas.github.io/blog/20170627/facebook-infrastructure-streaming-video-engine-sve.html"/>
    <updated>2017-06-27T22:16:02-07:00</updated>
    <id>http://euccas.github.io/blog/20170627/facebook-infrastructure-streaming-video-engine-sve</id>
    <content type="html"><![CDATA[<p>In last year’s <a href="https://developers.facebook.com/videos/?category=f8_2016"><strong>Facebook F8 conference</strong></a>, Sachin Kulkarni, who worked on Facebook’s Video Infrastructure, gave a talk (<a href="https://developers.facebook.com/videos/f8-2016/inside-look-at-facebook-media-infrastructure/">watch it here</a>) to introduce the design of Facebook’s <strong>Streaming Video Engine System (SVE)</strong>. I found this talk particularly interesting because it revealed, in a very well structured, concise yet informative way, how Facebook infrastructure team came up with a solution to build a video system solving user frustrations by reviewing the end-to-end process, and how such a design meet the goal of being <strong>fast</strong>, <strong>flexible</strong>, <strong>scalable</strong>, and <strong>efficient</strong>. After watching the presentation video for a few times, I thought it would be helpful to write down some notes here, for my own reviewing in the future, and for people who might be interested in Facebook’s media infrastructure.</p>

<p>Sharing on Facebook started from largely text, and quickly changed to be largely photos. Since 2014, more videos started to be posted and shared among users. The challenge was, building a video processing system is much harder than building a text or image processing system. Videos are greedy, they will consume all your resources: CPU, memory, disk, network, and anything else.</p>

<p>Before building the Streaming Video Engine system, the team started by reviewing Facebook’s existing video uploading and processing process, which was slow and not scalable. They found several problems need change or improvement:</p>

<!--more-->

<ul>
  <li>No unified clients</li>
  <li>Several disk reads and writes in the critical path</li>
  <li>Was doing serial processing throughout</li>
  <li>Read a video as one single big file, instead of splitting it up to chunks</li>
</ul>

<p>The new Streaming Video Engine (SVE) is expected to solve the aforementioned problems, and to meet the four design goals:</p>

<ul>
  <li>Fast: make users upload their videos super fast</li>
  <li>Flexible: usable for different Facebook products</li>
  <li>Scalable: everything at Facebook has to scale</li>
  <li>Efficient: storage efficiency, processing efficiency, and more importantly consume less bytes of users’ data plan</li>
</ul>

<p>These four design goals, in my opinion, are also the most common goals applicable to most engineering infrastructure systems.</p>

<p>Let’s take a deep dive to see how SVE was designed to meet these goals.</p>

<h1 id="fast">Fast</h1>

<ul>
  <li>First step is build a common library (for video uploading) that could be used for the clients cross platforms (Web, Mobile, Android, etc.). With the common library, optimizations on video uploading can be applied to all platforms.</li>
  <li>The uploading library has functions to split a video by GOP (Group of Pictures, a GOP roughly is a scene in the video) alignment. So any given video can be split to segments, which can have multiple GOPs.</li>
  <li>Uploading process starts as soon as the clients split a video into segaments. The <strong>client</strong> uploads one segment a time to the <strong>web server</strong>.</li>
  <li>Web server sends out segments to the <strong>preprocessor</strong>, which is a write-through cache.</li>
  <li>Proprocessor handles:
    <ul>
      <li>Normalize the video (segment) if it needs to</li>
      <li>Notify the <strong>scheduler</strong> that there are video segments available to be encoded</li>
      <li>Write the video (segment) to the <strong>original storage</strong></li>
      <li>Further split the video segment into GOPs</li>
    </ul>
  </li>
  <li>Scheduler will find workers to encode videos. Multiple works can be utilized and each worker will process one or multiple GOPs.</li>
  <li>Overlapped upload and encoding process: While proprocessor, scheduler and works are working, the uploading process is still ongoing. Clients continues splitting videos into segments and uploading to the web server.</li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170627-fb_00.png" width="600"></p>

<p>With this design, the process speedup reached 2.3x (small videos &lt; 3MB) ~ 9.3x (large videos &gt; 1G).</p>

<h1 id="flexible">Flexible</h1>

<ul>
  <li>The key insight that allows SVE to be flexible is, all the video processing pipelines can be represented as a DAG (Directed Acyclic Graph).</li>
  <li>Arbitrary dependencies can be added between the tasks in the video processing pipepline. The added tasks can be executed in parallel while the main pipeline tasks are running.</li>
  <li>SVE provides very simple API functions for the video pipeline (Ideally, you can add a video processing pipeline in your product in less than 10 lines of code).</li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170627-fb_01.png" width="600"></p>

<h1 id="scalable">Scalable</h1>

<ul>
  <li>SVE was designed to prepare for overloads, such as handling the worldwide uploading “spike” on New Year’s Eve (could be 3x video uploads).</li>
  <li>Building a scalable system is relevant only when the system is <strong>robust</strong>. When the system gets overloaded, it must <strong>gracefully degrade</strong>. It cannot crash and burn.</li>
  <li>Prepare for overload along two dimensions: at the pipeline level, and the task level.</li>
  <li>Pipeline level, when uploads overwhelm the system:
    <ul>
      <li>Do not cache original videos in upload: Preprocessor stops caching original videos. Workers then need fetch videos from the original storage, not from preprocessor. The cost here is disk latency is added to the critical path.</li>
      <li>Delay pipeline generation for incoming video. Distinguish the critical video pipeline requests and the non-critical ones, then delay the non-critical ones.</li>
      <li>Reroute traffic to a different (less busy) region (Asia, Europe, US west, etc.)</li>
    </ul>
  </li>
  <li>Task level (the tasks executed by <strong>workers</strong> in the pipeline), when too many tasks are running:
    <ul>
      <li>Push back non latency-sensitive jobs</li>
      <li>Turn off A/B tests, which try to figure out the best encoding for the given video</li>
      <li>Add more workers (this requires making it easy to add capacity to SVE)</li>
    </ul>
  </li>
</ul>

<h1 id="efficient">Efficient</h1>

<ul>
  <li>The high level problem statement here is: If we could use 100% CPU, how can we make the encoded video as small as possible?</li>
  <li>Find the optimal encoding settings to get the best balance between encoded video file size and time spent on encoding. The difficult part is modern encoders can have hundreds of settings for one video. Chance of picking optimal combination is extremely low.</li>
  <li>The adopted solution is:
    <ul>
      <li>Categorize each scene such as “minimal motion”, “rapid movement”, and “complex crowded scene”.</li>
      <li>Build a Neural Network Model and a large training data set to train the network.</li>
      <li>In SVE, video scene segments are sent to a Fingerprint generator, which generates fingerprints and sends them to the Neural Network Model.</li>
      <li>The neural network figures out optimal encoding settings (could be multiple) for each scene, and sends the encoding settings to encoders.</li>
      <li>the encoder takes the settings, and encodes the video scenes in multiple ways. Then discard the encoded videos which are below quality bar.</li>
    </ul>
  </li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170627-fb_02.png" width="600"></p>

<p>SVE achieved 20% smaller video file sizes. This is a huge saving of user’s data plans.</p>

<p>This Streaming Video Engine was designed, coded and tested in roughly 9 months. The most important learnings are:</p>

<ul>
  <li>E2E view: To find an optimal solution, we need look at the flow end to end</li>
  <li>Multi-dimensional flexibility is a key for making the system most useful</li>
  <li>Parallel and shadow mode testing to find correctness and scalability issues before production</li>
  <li>Design the ability to handle extreme products such as 360 videos</li>
  <li>Track direct measures (latency, reliability, etc.) and indirect measures (number of videos uploaded, watch times, etc.). Mapping indirect measures to direct measures could give you a good view in figuring out what you could do better next.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Instagram Moved to Python 3]]></title>
    <link href="http://euccas.github.io/blog/20170616/how-instagram-moved-to-python-3.html"/>
    <updated>2017-06-16T17:52:34-07:00</updated>
    <id>http://euccas.github.io/blog/20170616/how-instagram-moved-to-python-3</id>
    <content type="html"><![CDATA[<p>Instagram, the famous brunch sharing app, presented in <a href="https://us.pycon.org/2017/">PyCon 2017</a> and gave a talk in the keynote session on “How Instagram moves to Python 3”. If you have 15 minutes, read the interview with the speakers, Hui Ding and Lisa Guo from Instagram Infrastructure team, <a href="https://thenewstack.io/instagram-makes-smooth-move-python-3/]"><strong>here</strong></a>. If you have 45 minutes, watch their PyCon talk video, <a href="https://www.youtube.com/watch?v=66XoCk79kjM"><strong>here</strong></a>. If you have only 5 minutes, continue reading, <strong>right here</strong>.</p>

<p>Instagram’s backend, which serves over 400 million active users every day, is built on Python/Django stack. The decision on whether moving from Python 2 to Python 3, was really a decision on whether investing in a version of the language that was mature, but wasn’t going anywhere (Python 2 is expected to retire in 2020) – or the language that was the next version and had great and growing community support. The major motivations behind Instagram’s migration to Python 3 are:</p>

<ul>
  <li><strong>Typing support</strong> for dev velocity</li>
  <li>Better <strong>performance</strong> than Python 2</li>
  <li><strong>Community</strong> continues to make Python 3 better and faster</li>
</ul>

<p>The whole migration process took about 10 months, in roughly 3 stages.</p>

<!--more-->

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_00.png" width="520"></p>

<ul>
  <li>First off, the migration was done directly on the Master Branch, which means the developers were adding new features to the code while migration was ongoing. So in the beginning of the Mirgration process, infrastructure added support of Python 3 on the Master Branch to make the code be able to run with both Python 2 and Python 3 environment.</li>
  <li>Massive code modification for 3 months, with the help of Python package <a href="https://pypi.python.org/pypi/modernize"><strong>“modernize”</strong></a>. Meanwhile, upgraded Third-party packages to Python 3 (working rule: <em>No Python 3, no new package</em>). Also deleted unused, incompatible packages.</li>
  <li>Intensive unit testing for 2 months. One limitation is data compatibility issues typically do not show up in unit tests.</li>
  <li>Production rollout for another 4 months (push Python 3 to every developer’s sandbox)</li>
</ul>

<p>In the talk, Lisa shared the challenges they faced in the migration process and how did they solved those problems.</p>

<ul>
  <li>Differences in <strong>unicode</strong>, <strong>str</strong>, <strong>bytes</strong>. Solved by using helper functions.</li>
  <li><strong>Pickle memcache data format incompatibility</strong> in Python 2 and Python 3. Solved by isolating memcaches for Python 2 and Python 3.</li>
  <li><strong>Iterator</strong> differences, such as <code>map</code>. Solved by converting all maps to list in Python 3.</li>
  <li><strong>Dictionary order</strong> is different in different Python versions, which caused differences in the dumped JSON data. Solved by forcing <code>sorted_keys</code> in <code>json.dump</code> function.</li>
  <li>With Python 3, while CPU instructions per request decreased by 12%, max requests per second (capacity) had 0% increase! Found the root cause in the code of checking memory configuration, and the issue was memory optimization condition was never met in Python 3 as <code>True</code> because of unicode issue. Solved by adding a magical character <strong>“b”</strong>, just like this:</li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_01.png" width="520"></p>

<p>In Feb 2017, Instagram’s stack completely dropped Python 2 and moved to Python 3 (v3.6). So far they’ve got this from Python 3:</p>

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_02.png" width="520"></p>

<p>One more thing, in the talk Hui Ding also briefly discussed a few <strong>Python Efficiency Strategies</strong> that Instagram used to support the growing number of features and users:</p>

<ul>
  <li>Build extensive tools to profile and understand perf bottleneck</li>
  <li>Proactively push stable, critical components to C/C++, e.g., memcached access library</li>
  <li>Use Cythonization to improve performance</li>
  <li>Future ideas: Make the Django stack completely Async? Create a new python runtime?</li>
</ul>

<p>Changing an existing service to use a new version of language can never be easy, especially when your service is at such a scale - serving millions of people. You just cannot afford to breaking the existing service. Moving to Python 3 in 10 months must be a challenging process. “It can be done. It worths it. Make it happen. And Make Python 3 better.”</p>

<p>Nice work Instagram!</p>

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_03.png" width="520"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Levels of Design]]></title>
    <link href="http://euccas.github.io/blog/20161224/levels-of-design.html"/>
    <updated>2016-12-24T09:52:07-08:00</updated>
    <id>http://euccas.github.io/blog/20161224/levels-of-design</id>
    <content type="html"><![CDATA[<p>Recently I’m taking a course <a href="https://www.coursera.org/learn/algorithmic-toolbox/home/welcome">“Algorithm Toolbox”</a> on Coursera. This course provides me a good chance to review and enhance my knowledge in the fundamental algorithms, which usually would help on achieving better system design. This morning I came across one slide of this course and thought it could be very useful. Sharing it here.</p>

<p><img class="center" src="/images/post_images/2016/20161224-levels-of-design-1.png" width="600"></p>

<p>It’s important to keep the <strong>levels of (algorithm) design</strong> in mind when solving a problem.</p>

<!--more-->

<h1 id="level-1-naive-algorithm">Level 1: Naive Algorithm</h1>

<p>This is the solution that you can get just by taking the definition of a problem and turning it into an algorithm. This solution can solve the problem, but it is often very slow and inefficient.</p>

<p>The way I see the naive algorithm is it gives you something that works, and might be used to verify if alternative solutions are correct or not. But it’s important to not stay at the this solution. You should keep looking for better solutions.</p>

<h1 id="level-2-algorithm-by-way-of-standard-tools">Level 2: Algorithm by way of standard Tools</h1>

<p>You can look at the standard techniques and see if any of it applies to solving your problem. On this level, your goal is finding some standard techniques that work, often that don’t involve too much effort on your part, and give you something that work very well (better than the naive algorithm).</p>

<h1 id="level-3-optimized-algorithm">Level 3: Optimized Algorithm</h1>

<p>Remember there are always lots of ways to improve an existing solution. If you get a pretty good solution on level 2, why not taking one more step and see what you can do to improve it? Could you reduce the runtime from n-cubed to n-squared or n-squred to n? Could you come up with a shorter solution by rearranging the order or cut out some of the work? Could you use a data structure to speed things up? Think about all these possiblities and see if you can get a even better solution. (Often you will)</p>

<h1 id="level-4-magic-algorithm">Level 4: Magic Algorithm</h1>

<p>When the solutions from the previous three levels are not good enough, you’ll need some magic to get a better one. This can be hard. You will need some clever new ideas and unique insights of the problem you’re trying to solve. Even if you don’t get a magic solution in the end, the thought process will be beneficial.</p>

<p>Sometimes when I finish a project, I do have the feeling that the way I do it is just not good enough, even though the project has been proved to be successful, useful and solved a particular technical challenge we faced. Looking at the “Levels of Design”, I believe what I need to do is spending more efforts on the higher levels. Actually what you really need for getting a really good solution is not magic. What you need is mastering more existing good standard techniques, exploring the possibility for optimization, and a deep understanding of the problem you want to solve.</p>

]]></content>
  </entry>
  
</feed>
